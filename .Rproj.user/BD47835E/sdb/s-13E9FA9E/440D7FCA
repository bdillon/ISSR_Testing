{
    "contents" : "rm(list = ls())\nlibrary(stringr)\nlibrary(scrapeR)\nlibrary(ggplot2)\n\n###### Fxns #######\n\nScrapePage <- function(cur.url) {\n  cat(cur.url, \"\\n\")\n  cur.url <- tolower(cur.url)\n  cur.page <- getURL(cur.url)\n  cur.page <- str_split(cur.page, '\\n')[[1]]\n  start <- grep(\"112th CONGRESS\", cur.page)\n  end <- grep(\"&lt;all&gt;\", cur.page)\n  if(length(end) > 0 & length(start) > 0){\n    # Get just the text\n    print(start)\n    print(end)\n    if(!is.na(start) & !is.na(end)){\n      if(start < end & start > 0 & end > 0){\n        bill.text <- cur.page[start:end]\n      }else{\n        bill.text <- \"\"\n      }\n    }else{\n      bill.text <- \"\"\n    }\n  }else{\n    bill.text <- \"\"\n  }\n  to_return <- list(page = cur.page, text = bill.text)\n  Sys.sleep(5)      ## Important: so we don't cause a DOS attack\n  return(to_return)\n}\n\nTokenizeString <- function(string){\n  temp <- tolower(string)\n  temp <- stringr::str_replace_all(temp,\"[^a-zA-Z\\\\s:\\\\?\\\\!]\", \" \")\n  temp <- stringr::str_replace_all(temp,\"[\\\\s]+\", \" \")\n  temp <- stringr::str_split(temp, \" \")[[1]]\n  indexes <- which(temp == \"\")\n  if(length(indexes) > 0){\n    temp <- temp[-indexes]\n  }\n  return(temp)\n}\n\nParseBill <- function(cur.bill) {\n  cur.page <- cur.bill$page\n  if (length(grep(\"dc.creator\",cur.page)) > 0) {\n    cur.creator <- str_split(cur.page[grep(\"dc.creator\",cur.page)],\"content=\\\"\")[[1]][2]\n    cur.bill$creator <- str_split(cur.creator,\"\\\"\")[[1]][1]\n  } else {\n    cur.bill$creator <- \"None\"\n  }\n  return(cur.bill)\n}\n\nTokenizeBill <- function(cur.bill) {\n  cur.text <- cur.bill$text\n  tokenized.text <- unlist(\n    lapply(cur.text, function(x) TokenizeString(x))\n  )\n  cur.bill$tokenized <- tokenized.text\n  cur.bill$N <- length(tokenized.text)\n  cur.bill$types <- length(unique(tokenized.text))\n  cur.bill$word.counts <- xtabs(~tokenized.text)\n  return(cur.bill)\n}\n\nGetWordCount <- function(cur.word, all.bills) {\n  all.counts <- {}\n  for (bill in all.bills) {\n    if (is.na(bill$word.counts[cur.word])) {\n      cur.count <- 0 \n    } else {\n      cur.count <- bill$word.counts[cur.word]\n    }\n    all.counts <- c(all.counts,cur.count)\n  }  \n  return(sum(all.counts))\n}\n\nMakeDataFrame <- function(cur.bill) {\n  if (cur.bill$text != \"\"){\n    cur.summary <- data.frame(\n                     N = cur.bill$word.counts,\n                     Author = cur.bill$creator\n                      )\n    colnames(cur.summary) <- c(\"Word\",\"Freq\",\"Author\")\n  } else {\n    cur.summary <- {}\n  }\n  return(cur.summary)\n}\n\n###### Body #######\n\n###### Load data\nload(\"./Data/Bill_URLs.Rdata\")\n\n###### Process URLs; update applied 6/1/2015\n\nbill.urls <- sapply(Bill_URLs, \n  function(x) str_replace_all(x, 'http://beta.', 'https://www.'),\n  simplify=TRUE)\n\nbill.urls <- sapply(bill.urls, \n  function(x) paste(x, \"/text?format=txt\", sep=\"\"),\n  simplify=TRUE)\n\n###### Process text\n\nbills.raw <- lapply(bill.urls, ScrapePage)\nbills.tokenized <- lapply(bills.raw, TokenizeBill)\nbills.processed <- lapply(bills.tokenized, ParseBill)\n\n###### Unwrap into data.frame object\n\nbills.summary <- {}\nfor (bill in bills.processed) {\n  bills.summary <- rbind(bills.summary,MakeDataFrame(bill))  \n}\n\n###### Histogram plots of:\n######    \"defense\" by author\n######    \"eduction\" by author\n\nggplot(subset(bills.summary,Word == \"defense\" | Word == \"education\"),\n              aes(y = Freq, x = Word)) + geom_bar(stat='identity') + facet_wrap(~Author)\n\n\n",
    "created" : 1433206663420.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "286314639",
    "id" : "440D7FCA",
    "lastKnownWriteTime" : 1433206783,
    "path" : "~/Documents/ISSR_Testing/Scripts/DayOne_Webscraping.R",
    "project_path" : "Scripts/DayOne_Webscraping.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}