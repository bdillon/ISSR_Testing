cat(paste("Hello,", "I have", fingers, "Fingers", sep = " "))
#however, with cat, I can just skip the paste part and it will print the stuff directly
cat("Hello,", "I have", fingers, "Fingers")
fingers <- 10
cat(paste("Hello,", "I have", fingers, "Fingers", sep = " "))
#however, with cat, I can just skip the paste part and it will print the stuff directly
cat("Hello,", "I have", fingers, "Fingers")
install.packages('stringr')
install.packages('scrapeR')
rm(list = ls())
getwd()
print("Hello World")
cat("Hello World")
{
cat("Hello")
cat("World")
}
{
print("Hello")
print("World")
}
{
cat("Hello ")
cat("World")
}
{
cat("Hello \n")
cat("World")
}
{
cat("Hello\n")
cat("World")
}
fingers <- 10
#now lets print out how many fingers we have:
print(paste("Hello,", "I have", fingers, "Fingers", sep = " "))
print(paste("Hello,", "I have", fingers, "Fingers", sep = "-----"))
cat("Hello,", "I have", fingers, "Fingers")
?cat
cat("My Grocery List:\n", "1 dozen eggs\n","1 loaf of bread\n 1 bottle of orange juice\n", "1 pint mass mocha")
my_vector <- c(20:30)
# take a look
cat(my_vector)
for(i in 1:length(my_vector)){
my_vector[i] <- sqrt(my_vector[i])
}
cat(my_vector)
?for
??
??for
/
for(i in my_vector){
cat(i)
}
for(i in my_vector){
print(i)
}
my_number <- 19
if(my_number < 20){
cat("My number is less than 20")
}
an example to check and see if our number is equal to 20
my_number <- 19
if(my_number < 20){
cat("My number is less than 20")
}
my_number <- 22
if(my_number < 20){
cat("My number is less than 20")
}
MyColumnSum <- function(col.number, my.matrix ){
#take the column sum of the matrix
col.sum <- sum(my.matrix[, col.number])
return(col.sum)
}
my.mat <- matrix(1:100,nrow=10,ncol=10)
my.mat
MyColumnSum(1, mymat)
MyColumnSum(1, my.mat)
MyColumnSum(2, my.mat)
MyColumnSum(3, my.mat)
MyColumnSum(6, my.mat)
way.
library(scrapeR)
library(stringr)
library(scrapeR)
library(stringr)
GetGoogleScholarResults <- function(string, return_source = FALSE) {
# print out the input name
cat(string, "\n")
# make the input name all lowercase
string <- tolower(string)
# split the string on spaces
str <- str_split(string, " ")[[1]]
# combine the resulting parts of the string with + signs so "Matt Denny" will end up as "matt+denny" which is what Google Scholar wants as input
str <- paste0(str, collapse = "+")
# add the name (which is now in the correct format) to the search querry and we have our web address.
str <- paste("https://scholar.google.com/scholar?hl=en&q=", str, sep = "")
# downloads the web page source code
page <- getURL(str)
# search for the 'Scholar</a><div id="gs_ab_md">' string which occurs uniquely right before google Scholar tells you how many results your querry returned
num_results <- str_split(page,'Scholar</a><div id=\\"gs_ab_md\\">')[[1]][2]
# split the resulting string on the fist time you see a "(" as this will signify the end of the text string telling you how many results were returned.
num_results <- str_split(num_results,'\\(')[[1]][1]
# Print out the number of results returned by Google Scholar
cat("Querry returned", tolower(num_results), "\n")
# Look to see if the "User profiles" string is present -- grepl will return true if the specified text ("User profiles") is contained in the web page source.
if(grepl("User profiles",page)){
# split the web page source (which is all one string) on the "Cited by " string and then take the second chunk of the resulting vector of substrings (so we can get at the number right after the first mention of "Cited by ")
num_cites <- str_split(page,"Cited by ")[[1]][2]
# now we want the number before the < symbol in the resulting string  (which will be the number of cites)
num_cites <- str_split(num_cites,"<")[[1]][1]
# now let the user know how many we found
cat("Number of Cites:",num_cites,"\n")
}else{
# If we could not find the "User profiles" string, then the person probably does not have a profile on Google Scholar and we should let the user know this is the case
cat("This user may not have a Google Scholar profile \n")
}
# If we specified the option at the top that we wanted to return the HTML source, then return it, otherwise don't.
if(return_source){
return(page)
}
}
GetGoogleScholarResults("Brian Dillon")
GetGoogleScholarResults("Laurel Smith-Doerr")
GetGoogleScholarResults("Brian W Dillon")
GetGoogleScholarResults("Matt Wagers")
GetGoogleScholarResults("Lyn Frazier")
str <- paste('https://blogs.umass.edu/bwdillon')
page <- getURL(str)
page
?getURL
str
getURL("http://www.cnn.com")
getURL(str)
getURL("http://www.cnn.com")
?scrapeR
GetGoogleScholarResults <- function(string, return_source = FALSE) {
# print out the input name
cat(string, "\n")
# make the input name all lowercase
string <- tolower(string)
# split the string on spaces
str <- str_split(string, " ")[[1]]
# combine the resulting parts of the string with + signs so "Matt Denny" will end up as "matt+denny" which is what Google Scholar wants as input
str <- paste0(str, collapse = "+")
# add the name (which is now in the correct format) to the search querry and we have our web address.
str <- paste("https://scholar.google.com/scholar?hl=en&q=", str, sep = "")
# downloads the web page source code
page <- getURL(str)
# search for the 'Scholar</a><div id="gs_ab_md">' string which occurs uniquely right before google Scholar tells you how many results your querry returned
num_results <- str_split(page,'Scholar</a><div id=\\"gs_ab_md\\">')[[1]][2]
# split the resulting string on the fist time you see a "(" as this will signify the end of the text string telling you how many results were returned.
num_results <- str_split(num_results,'\\(')[[1]][1]
# Print out the number of results returned by Google Scholar
cat("Querry returned", tolower(num_results), "\n")
# Look to see if the "User profiles" string is present -- grepl will return true if the specified text ("User profiles") is contained in the web page source.
if(grepl("User profiles",page)){
# split the web page source (which is all one string) on the "Cited by " string and then take the second chunk of the resulting vector of substrings (so we can get at the number right after the first mention of "Cited by ")
num_cites <- str_split(page,"Cited by ")[[1]][2]
# now we want the number before the < symbol in the resulting string  (which will be the number of cites)
num_cites <- str_split(num_cites,"<")[[1]][1]
# now let the user know how many we found
cat("Number of Cites:",num_cites,"\n")
}else{
# If we could not find the "User profiles" string, then the person probably does not have a profile on Google Scholar and we should let the user know this is the case
cat("This user may not have a Google Scholar profile \n")
}
# If we specified the option at the top that we wanted to return the HTML source, then return it, otherwise don't.
if(return_source){
return(page)
}
}
#now lets have some fun...
get_google_scholar_results("Joya Misra")
GetGoogleScholarResults("Brian Schaffner")
GetGoogleScholarResults("Laurel Smith-Doerr")
GetGoogleScholarResults("Gary Becker")
GetGoogleScholarResults("Joya Misra")
str
page <- getURL(str, .opts = list(ssl.verifypeer = FALSE))
page
GetGoogleScholarResults("Brian Dillon")
GetGoogleScholarResults <- function(string, return.source = FALSE) {
# print out the input name
cat(string, "\n")
# make the input name all lowercase
string <- tolower(string)
# split the string on spaces
str <- str_split(string, " ")[[1]]
# combine the resulting parts of the string with + signs so "Matt Denny" will end up as "matt+denny" which is what Google Scholar wants as input
str <- paste0(str, collapse = "+")
# add the name (which is now in the correct format) to the search querry and we have our web address.
str <- paste("https://scholar.google.com/scholar?hl=en&q=", str, sep = "")
# downloads the web page source code
page <- getURL(str, .opts = list(ssl.verifypeer = FALSE))
# search for the 'Scholar</a><div id="gs_ab_md">' string which occurs uniquely right before google Scholar tells you how many results your querry returned
num.results <- str_split(page, 'Scholar</a><div id=\\"gs_ab_md\\">')[[1]][2]
# split the resulting string on the fist time you see a "(" as this will signify the end of the text string telling you how many results were returned.
num.results <- str_split(num.results, '\\(')[[1]][1]
# Print out the number of results returned by Google Scholar
cat("Query returned", tolower(num.results), "\n")
# Look to see if the "User profiles" string is present -- grepl will return true if the specified text ("User profiles") is contained in the web page source.
if (grepl("User profiles", page)) {
# split the web page source (which is all one string) on the "Cited by " string and then take the second chunk of the resulting vector of substrings (so we can get at the number right after the first mention of "Cited by ")
num.cites <- str_split(page, "Cited by ")[[1]][2]
# now we want the number before the < symbol in the resulting string  (which will be the number of cites)
num.cites <- str_split(num.cites, "<")[[1]][1]
# now let the user know how many we found
cat("Number of Cites:", num.cites, "\n")
} else {
# If we could not find the "User profiles" string, then the person probably does not have a profile on Google Scholar and we should let the user know this is the case
cat("This user may not have a Google Scholar profile \n")
}
# If we specified the option at the top that we wanted to return the HTML source, then return it, otherwise don't.
if(return.source){
return(page)
}
}
GetGoogleScholarResults("Brian Dillon")
GetGoogleScholarResults("Colin Phillips")
GetGoogleScholarResults("Wing Yee Chow")
GetGoogleScholarResults("Ellen Lau")
GetGoogleScholarResults("Noam Chomsky")
GetGoogleScholarResults("Brian McElree")
GetGoogleScholarResults("Christopher Potts")
GetGoogleScholarResults("Chris Potts")
GetGoogleScholarResults("Christopher Potts")
library(stringr)
#' Create a string
my.string <- "Example STRING, with numbers (12, 15 and also 10.2)?!"
cat(my.string, "\n")
#' lowercase it
lower.string <- tolower(my.string)
cat(lower.string, "\n")
#' lets paste two strings together
second.string <- "Wow, two sentences."
my.string <- paste(my.string, second.string, sep = " ")
cat(my.string, "\n")
my.string.vector <- str_split(my.string, "!")[[1]]
print(my.string.vector)
grep("\\?", my.string.vector)
grepl("\\?",my.string.vector[1])
str_replace_all(my.string, "e", "___")
str_extract_all(my.string, "[0-9]+")
?str_split
str_split(my.string, "!")
str_split(c(my.string,my.string), "!")
grepl("\\?", my.string.vector[1])
str(str_split(my_vector))
str(str_split(my_vector,"?"))
str(str_split(my_vector,split = "?"))
str(str_split(my.string,"?"))
str(str_split(my.string,"\?"))
str_split(my.string,"\?")
str_split(my.string,"?")
str_split(my.string,"\\?")
str(str_split(my.string,"\\?"))
unique(rapply(t1, function(x) head(x, 1)))
?rapply
str_replace_all(my.string, "e", "___")
my.string.list <- c(my.string, my.string)
my.parse <- str_split(my.string.list,"!")
unique(rapply(my.parse,function (x) head(x, 1)))
rapply(my.parse,function (x) head(x, 1))
to your Github repo after completing each step.
rm(list = ls())
# load the necessary libararies
library(stringr)
library(scrapeR)
# the path below
load("./Data/Bill_URLs.Rdata")
ls()
Bill_URLs
function(x) str_replace_all(x,'http','https')
apply(Bill_URLs,function(x) str_replace_all(x,'http','https'))
?apply
lapply(Bill_URLs,function(x) str_replace_all(x,'http','https'))
vapply(Bill_URLs,function(x) str_replace_all(x, 'http', 'https'))
?apply
sapply(Bill_URLs,function(x) str_replace_all(x, 'http', 'https'))
?lapply
bill.urls <- lapply(Bill_URLs,function(x) str_replace_all(x, 'http', 'https'),simplify=TRUE)
bill.urls <- lapply(Bill_URLs,function(x) str_replace_all(x, 'http', 'https'),simplify=T)
bill.urls <- lapply(Bill_URLs,function(x) str_replace_all(x, 'http://beta.', 'https://www.'))
bill.urls
lapply(Bill_URLs,function(x) head(str_replace_all(x, 'http://beta.', 'https://www.'),1))
lapply(Bill_URLs,function(x) str_replace_all(x, 'http://beta.', 'https://www.')[[1]])
lapply(Bill_URLs,function(x) str_replace_all(x, 'http://beta.', 'https://www.')[[1]][1]
)
bill.urls <- sapply(Bill_URLs,function(x) str_replace_all(x, 'http://beta.', 'https://www.'),simplify=T)
bill.urls
bill.urls[1]
bill.urls <- sapply(bill.urls,
function(x) paste(x, "/text?format=txt"),
simplify=TRUE)
bill.urls
bill.urls <- sapply(bill.urls,
function(x) paste(x, "/text?format=txt", sep=""),
simplify=TRUE)
ScrapePage <- function(url) {
cat(url, "\n")
url <- tolower(url)
page <- getURL(url)
page <- str_split(page, '\n')[[1]]
start <- grep("112th CONGRESS", page)
end <- grep("&lt;all&gt;", page)
bill_text <- page[start:end]
to_return <- list(page = page, text = bill_text)
return(to_return)
}
ScrapePage <- function(cur.url) {
cat(cur.url, "\n")
cur.url <- tolower(cur.url)
cur.page <- getURL(cur.url)
cur.page <- str_split(cur.page, '\n')[[1]]
start <- grep("112th CONGRESS", cur.page)
end <- grep("&lt;all&gt;", cur.page)
bill_text <- cur.page[start:end]
to_return <- list(page = cur.page, text = bill_text)
return(to_return)
}
ScrapePage()
test <- ScrapePage( url = "https://www.congress.gov/bill/112th-congress/senate-bill/886/text?format=txt")
test <- ScrapePage(cur.url = "https://www.congress.gov/bill/112th-congress/senate-bill/886/text?format=txt")
cur.page <- getURL(cur.url)
cur.url = "https://www.congress.gov/bill/112th-congress/senate-bill/886/text?format=txt"
cur.page <- getURL(cur.url)
cur.page
print(Test)
print(test)
